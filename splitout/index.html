<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SPLITOUT: Out-of-the-Box Training-Hijacking Detection">
  <meta property="og:title" content="SPLITOUT: Out-of-the-Box Training-Hijacking Detection"/>
  <meta property="og:description" content="SPLITOUT: Out-of-the-Box Training-Hijacking Detection in Split Learning via Outlier Detection"/>
  <meta property="og:url" content="starkslab.github.io/splitout"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/header.png" />
  <meta property="og:image:width" content="630"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="SPLITOUT: Out-of-the-Box Training-Hijacking Detection">
  <meta name="twitter:description" content="SPLITOUT: Out-of-the-Box Training-Hijacking Detection">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/header.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="split learning, training hijacking, outlier detection, anomaly detection">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SplitOut: Out-of-the-Box Training-Hijacking Detection in Split Learning via Outlier Detection</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SplitOut: Out-of-the-Box Training-Hijacking Detection in Split Learning via Outlier Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><sup>1,* </sup><a href="https://scholar.google.com/citations?user=HjcFsdcAAAAJ&hl=en" target="_blank">Ege Erdoğan</a>,</span>
              <span class="author-block"><sup>2,* </sup><a href="https://scholar.google.com/citations?user=7Iyg4ZoAAAAJ&hl=en" target="_blank">Unat Tekşen</a>,</span>
              <span class="author-block"><sup>3,* </sup><a href="https://scholar.google.com/citations?user=QPX7vbgAAAAJ&hl=en" target="_blank">Mehmet Salih Çeliktenyıldız</a>,</span>
              <span class="author-block"><sup>4 </sup><a href="https://scholar.google.com/citations?user=lr806D4AAAAJ&hl=en" target="_blank">Alptekin Küpçü</a>,</span>
              <span class="author-block"><sup>3 </sup><a href="https://scholar.google.com/citations?user=VuSgyckAAAAJ&hl=en" target="_blank">A. Ercüment Çiçek</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Technical University of Munich, Munich, Germany,
                <sup>2</sup>Kadir Has University, Istanbul, Turkey,
                <sup>3</sup>Bilkent University, Ankara, Turkey,
                <sup>4</sup>Koç University, Istanbul, Turkey
                <span class="eql-cntrb"><small><br><i><sup>*</sup>Indicates Equal Contribution</i></small></span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><br>International Conference on Cryptology And Network Security 2024</span>
              <br>
            </div>

            <div class="header-logos" style="display: flex; justify-content: center; align-items: center;">
              <img src="static/images/header.png" class="interpolation-image" alt="" style="margin: 0 10px; width: 15%;"/>
              <img src="static/images/koc-crypto-sqr.png" class="interpolation-image" alt="" style="margin: 0 10px; width: 15%;"/>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2302.08618" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2302.08618" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary (p. 21)</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/ege-erdogan/splitout" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2302.08618" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Split learning lets clients train deep neural networks with a central server without sharing raw data.
              A malicious server can hijack training and reconstruct private data or plant backdoors in client models.
              Earlier defenses detect such training-hijacking attacks but rely on many heuristics and many tuned hyperparameters.
            </p>
            <p>
              SplitOut is a simple passive detector that runs on the client side.
              Clients briefly train the full model locally on a small fraction (e.g., 1%) of their private data and collect honest gradients.
              We train an off-the-shelf Local Outlier Factor (LOF) model on these gradients.
              During real split learning, LOF flags incoming server gradients as inliers (honest) or outliers (hijacked).
            </p>
            <p>
              We evaluate SplitOut on MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and EMNIST.
              It detects all existing split learning training-hijacking attacks we test, with almost-perfect true positive rates and near-zero false positives, often within the first epoch.
              The method needs only modest additional client compute and no attack-specific tuning.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3">SplitOut</h2>
            <div class="content has-text-justified">
              <p>
                SplitOut is a client-side, passive detector for training-hijacking attacks in split learning.
              </p>
              <ul>
                <li>Before split learning, the client simulates one epoch of end-to-end training on a small subset (e.g., 1%) of its private data.</li>
                <li>It records the gradients of the simulated server part; these are the <i>honest</i> gradients used to train LOF.</li>
                <li>During real split learning, each gradient sent by the server is fed into the trained LOF model.</li>
                <li>A sliding window of the last <i>w</i> LOF decisions is kept; if most are outliers, the client flags an attack and stops training.</li>
              </ul>
              <p>
                We use LOF with the maximum feasible number of neighbors, and we rely only on honest gradients collected once before deployment.
                No attacker behavior is modeled and no attack-specific features are needed.
              </p>

              <br>
              <img src="static/images/fsha_results_OD.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>

              <br><br>

              <img src="static/images/table_splitoutmain.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 50%;"/>
              <br>
              <p>
                On FSHA, SplitSpy, and backdoor attacks, SplitOut reaches true positive rate (TPR) ≈ 1 and very low false positive rate (FPR).
                On MNIST and Fashion-MNIST it detects all attacks with TPR = 1 and FPR = 0.
                On CIFAR-10 and CIFAR-100, FPR is small and drops to zero when we allocate more data to LOF training.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3">Detecting the Attack with SplitOut</h2>
            <div class="content has-text-justified has-text-centered">
              <p>
                SplitOut detects feature-space hijacking attacks very early in training.
                The detector fires long before the attacker can reconstruct readable private images or install a strong backdoor.
              </p>
              <img src="static/images/reconstruct_lossplots_V3_hor.drawio.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
              <br>
              <p>
                Reconstructions obtained when SplitOut is active have almost zero SSIM with the original inputs,
                compared to high SSIM when FSHA runs without detection.
              </p>
              <p>
                <i><b>Red lines</b> represent where the attack (FSHA) is detected.</i>
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3">Avoiding the Curse of Dimensionality</h2>
            <div class="content has-text-justified">
              <img src="static/images/tsne-fsha-backdoor.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 60%;"/>
              <br>
              <p>
                Gradients live in a very high-dimensional space, but honest and malicious gradients form very different neighborhoods.
                t-SNE plots show honest gradients forming a tight, dense cluster.
                Malicious gradients spread over a larger region and largely surround the honest cluster.
              </p>
              <p>
                This structure is exactly what local density-based methods such as LOF exploit.
                Honest points sit in dense neighborhoods; hijacked gradients have sparser and more mixed neighborhoods and receive higher LOF scores.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="content has-text-justified">
              <img src="static/images/l2dist.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 60%;"/>
              <br>
              <p>
                Pair-wise average L2 distances between sets of honest and malicious gradients on CIFAR-10 reveal the same picture:
              </p>
              <ul>
                <li>Honest gradients are closest to other honest gradients.</li>
                <li>Honest gradients are closer to each other than malicious gradients are to each other.</li>
                <li>Malicious gradients are closer to honest gradients than to other malicious ones, so their local neighborhoods are low-density and mixed.</li>
              </ul>
              <p>
                These geometric properties explain why a neighborhood-based outlier detector like LOF can separate honest and hijacked gradients
                despite the curse of dimensionality.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3">Additional Figures</h2>
            <div class="content has-text-justified">
              <h4 class="title is-4">F-MNIST t-SNE</h4>
              <img src="static/images/f-mnist-tsne.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 60%;"/>
              <br>
              <h4 class="title is-4">EMNIST t-SNE</h4>
              <img src="static/images/tsne-emnist.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 33%;"/>
              <br>
              <p>
                Similar t-SNE plots on F-MNIST and EMNIST show the same pattern:
                honest gradients cluster tightly, while malicious gradients cover a broader region around them.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3">Assumptions and Limitations</h2>
            <div class="content has-text-justified">
              <p>
                SplitOut assumes:
              </p>
              <ul>
                <li>Clients know the full model architecture or a good approximation of it.</li>
                <li>Clients can run one local epoch of end-to-end training on a small part of their data (e.g., 1–10%).</li>
                <li>Attacks consistently change the distribution of server-side gradients over time.</li>
              </ul>
              <p>
                We evaluate feature-space alignment attacks: FSHA, SplitSpy, and a backdoor attack, plus a multitask FSHA that mixes honest and adversarial losses.
                SplitOut also detects the adaptive SplitSpy attacker that is designed to bypass SplitGuard.
              </p>
              <p>
                Limitations include possible new training-hijacking strategies that do not rely on feature-space alignment
                and attacks that start only after several epochs of honest training.
                Detecting such attacks may require more LOF training data or different detectors and is an open direction for future work.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@misc{erdogan2024splitoutoutoftheboxtraininghijackingdetection,
  title={SplitOut: Out-of-the-Box Training-Hijacking Detection in Split Learning via Outlier Detection},
  author={Ege Erdogan and Unat Teksen and Mehmet Salih Celiktenyildiz and Alptekin Kupcu and A. Ercument Cicek},
  year={2024},
  eprint={2302.08618},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2302.08618}
}
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="content has-text-justified">
              <br>
              <p>
                Check related research:
              </p>
              <ul>
                <li><a href="https://dl.acm.org/doi/pdf/10.1145/3559613.3563201">UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks Against Split Learning</a></li>
                <li><a href="https://dl.acm.org/doi/pdf/10.1145/3559613.3563198">SplitGuard: Detecting and Mitigating Training-Hijacking Attacks in Split Learning</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the design of this website, we just ask that you link back to this page in the footer. <br>
              This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

  </html>
